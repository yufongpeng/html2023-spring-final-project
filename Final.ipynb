{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqDuEVzvTDTu"
      },
      "source": [
        "# Package Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc7PKFk0zIna"
      },
      "outputs": [],
      "source": [
        "### import\n",
        "import random as rand\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict, RandomizedSearchCV, GridSearchCV, train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, ConfusionMatrixDisplay\n",
        "\n",
        "from scipy.stats import iqr, uniform\n",
        "!pip install scikit-optimize\n",
        "from skopt import BayesSearchCV\n",
        "import xgboost as xgb\n",
        "import joblib   \n",
        "#joblib.dump(best_model, 'best_model.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqgWUGvTTOAV"
      },
      "source": [
        "# Load Data & Preprocess Func\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lhgxBmg2_wLn"
      },
      "outputs": [],
      "source": [
        "### missing value\n",
        "# x should be pandas dataframe\n",
        "def KNN_Impute(x, k):\n",
        "    knn_impute = KNNImputer(n_neighbors=k) # n_neighbors, weights\n",
        "    x = knn_impute.fit_transform(x)\n",
        "    for i in range(x.shape[0]):\n",
        "        x[i][1] = round(x[i][1])\n",
        "    return x\n",
        "\n",
        "def KNN_Impute_iqrs(x, k):\n",
        "    iqrs = x.apply(lambda x: np.nanquantile(x, 0.75) - np.nanquantile(x, 0.25))\n",
        "    if x.shape[1] == 17:\n",
        "        iqrs[13:16] = 1\n",
        "    x = x / iqrs\n",
        "    knn_impute = KNNImputer(n_neighbors=k) # n_neighbors, weights\n",
        "    x = knn_impute.fit_transform(x)\n",
        "    x = x * iqrs.to_numpy()\n",
        "    for i in range(x.shape[0]):\n",
        "        x[i][1] = round(x[i][1])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Rufi6RuECQHX"
      },
      "outputs": [],
      "source": [
        "### load training data with pandas\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/train.csv\", delimiter=\",\", header=0)\n",
        "n_train = train_df.shape[0]\n",
        "\n",
        "# y_train\n",
        "y_train_pd = train_df[['Danceability']].copy()\n",
        "y_train    = y_train_pd.to_numpy()\n",
        "y_train    = np.reshape(y_train, n_train)\n",
        "\n",
        "train_df = train_df.drop(['Danceability','id','Track','Album','Uri','Url_spotify','Url_youtube','Description','Title','Channel'], axis =1)\n",
        "album_map = {'album': 3, 'single': 2, 'compilation': 1}\n",
        "tf_map    = {1: 1, 0: 0}\n",
        "train_df['Album_type']     = train_df['Album_type'].map(album_map, na_action='ignore')\n",
        "train_df['official_video'] = train_df['official_video'].map(tf_map, na_action='ignore')\n",
        "train_df['Licensed']       = train_df['Licensed'].map(tf_map, na_action='ignore')\n",
        "\n",
        "# x_train125: one hot encoding of artist, composer, album_type\n",
        "x_train_pd125 = pd.get_dummies(train_df, columns=['Artist','Composer','Album_type'])\n",
        "x_train125    = x_train_pd125.to_numpy()  \n",
        "# x_train17: categorical album_type, no artist, composer\n",
        "x_train_pd17 = train_df.drop(['Composer','Artist'], axis =1)\n",
        "x_train17 = x_train_pd17.to_numpy()\n",
        "# x_train14: no album_type, official_video, licensed, artist, composer\n",
        "x_train_pd14 = x_train_pd17.drop(['Album_type','official_video','Licensed'], axis =1)\n",
        "x_train14    = x_train_pd14.to_numpy()\n",
        "#pd.set_option('display.max_columns', 500)                                      \n",
        "#print(x_train_pd17.head(3))\n",
        "#print(x_train125.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U32P3LNqDyL"
      },
      "outputs": [],
      "source": [
        "### Impute\n",
        "x_train17_knn  = KNN_Impute_iqrs(x_train_pd17, 5)\n",
        "x_train14_knn  = KNN_Impute_iqrs(x_train_pd14, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFoPts6b376S"
      },
      "source": [
        "# Evaluation Func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tGsli1vwL4tB"
      },
      "outputs": [],
      "source": [
        "### Search Result\n",
        "def Search_Result(res):\n",
        "    print(\"mean\")\n",
        "    print(res.cv_results_['mean_test_score'])\n",
        "    print(\"std\")\n",
        "    print(res.cv_results_['std_test_score'])\n",
        "    print(\"rank\")\n",
        "    print(res.cv_results_['rank_test_score'])\n",
        "    print(res.best_params_)\n",
        "    print(res.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "T2oxVH5p3_vm"
      },
      "outputs": [],
      "source": [
        "### CV Interpretation\n",
        "def CV_Average(score, msg):\n",
        "    fold = score.shape[0]\n",
        "    sum = 0\n",
        "    for f in range(fold):\n",
        "        sum += score[f]\n",
        "    print(msg)\n",
        "    print('average: ' + str(sum/fold))\n",
        "    print('indiv.: '+str(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zbx3wTqq4S2e"
      },
      "outputs": [],
      "source": [
        "### Output Manipulation\n",
        "# Rounding\n",
        "def Reg_for_Cla(y):\n",
        "    y = y.round()\n",
        "    for i in range(y.shape[0]):\n",
        "        for j in range (y.shape[1]):\n",
        "            if   y[i][j] < 0:\n",
        "                y[i][j] = 0\n",
        "            elif y[i][j] > 9:\n",
        "                y[i][j] = 9\n",
        "    return y\n",
        "\n",
        "# Decision Stump\n",
        "def Stump(y_reg, y_cla):\n",
        "    n = y_reg.shape[0]\n",
        "    y_reg = y_reg.reshape((n,1))\n",
        "    y_cla = y_cla.reshape((n,1))\n",
        "    y = np.concatenate((y_reg, y_cla), axis=1)\n",
        "    y = y[y[:, 0].argsort()]\n",
        "\n",
        "\n",
        "    thr = np.empty(9)\n",
        "    for t in range(9):\n",
        "        y2 = np.copy(y)\n",
        "        for i in range (n):\n",
        "            if y[i][1] <= t:\n",
        "                y2[i][1] = -1\n",
        "            else:\n",
        "                y2[i][1] = 1\n",
        "\n",
        "        gtrain = np.zeros((n))\n",
        "        for i in range(n):\n",
        "            if y2[i][1] == -1:\n",
        "                gtrain[0] += 1\n",
        "        for i in range(1, n):\n",
        "            if y2[i-1][1] == -1:\n",
        "                gtrain[i] = gtrain[i-1] - 1;\n",
        "            else:                  \n",
        "                gtrain[i] = gtrain[i-1] + 1;\n",
        "\n",
        "        ming = 0\n",
        "        for i in range(n):\n",
        "            if gtrain[i] < gtrain[ming]:\n",
        "                ming = i\n",
        "        if ming == 0:  \n",
        "            g = -1;\n",
        "        else:\n",
        "            g = (y[ming][0] + y[ming-1][0]) / 2;\n",
        "\n",
        "        thr[t] = g\n",
        "        #print(str(t)+': '+str(gtrain[ming]/n))\n",
        "    return thr\n",
        "\n",
        "def Stump_Apply(y, stump):\n",
        "    y_pred = np.copy(y)\n",
        "    for i in range(y_pred.shape[0]):\n",
        "        if y_pred[i] < stump[0]:\n",
        "            y_pred[i] = 0\n",
        "        elif stump[8] < y_pred[i]:\n",
        "            y_pred[i] = 9\n",
        "        else:\n",
        "            for t in range (1, 9):\n",
        "                if stump[t-1] < y_pred[i] <= stump[t]:\n",
        "                    y_pred[i] = t\n",
        "    return y_pred\n",
        "\n",
        "# Stump()+Stump_Apply()\n",
        "def Stump_Set(y_test, y_train_pred, y_train_true):\n",
        "    stump = Stump(y_train_pred, y_train_true)\n",
        "    y_test_new = Stump_Apply(y_test, stump)\n",
        "    return y_test_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ABrxZ4dTUeW"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TxonjBbTYNp"
      },
      "source": [
        "[Hist Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor)\n",
        "\n",
        "[XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kHlhxBO0WKKC"
      },
      "outputs": [],
      "source": [
        "### XGBoost\n",
        "x_train = x_train125\n",
        "\n",
        "xgb_reg = xgb.XGBRegressor(gamma=0.001, learning_rate=0.048391470778895496, max_depth=9, min_child_weight=12.512400985268663, n_estimators=1456)\n",
        "\n",
        "#consruct bayesSearch object\n",
        "y_pred = cross_val_predict(xgb_reg, x_train, y_train, cv = 3)\n",
        "\n",
        "y_pred_int = y_pred.round()\n",
        "print(mean_absolute_error(y_pred_int, y_train))\n",
        "#[('gamma', 0.001), ('learning_rate', 0.048391470778895496), ('max_depth', 9), ('min_child_weight', 12.512400985268663), ('n_estimators', 1456)] -------> 1.5835760046592895"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIhOthwdoXyZ"
      },
      "outputs": [],
      "source": [
        "### Gradient Boosting Decision Tree\n",
        "x_train = x_train17 # x_train17_knn\n",
        "\n",
        "gbr   = make_pipeline(HistGradientBoostingRegressor(loss='absolute_error', categorical_features=[13, 14, 15]))\n",
        "gbr_p = make_pipeline(HistGradientBoostingRegressor(loss='absolute_error', categorical_features=[13, 14, 15], \n",
        "                                                    min_samples_leaf=150, max_leaf_nodes=33, max_depth=14, max_bins=225, learning_rate=0.1, l2_regularization=0.1))\n",
        "gbr_r = TransformedTargetRegressor(regressor=HistGradientBoostingRegressor(loss='absolute_error', categorical_features=[13, 14, 15], random_state=6211), inverse_func=Reg_for_Cla, check_inverse=False)\n",
        "gbr_rp= TransformedTargetRegressor(regressor=HistGradientBoostingRegressor(loss='absolute_error', categorical_features=[13, 14, 15], random_state=6211, \n",
        "                                                    min_samples_leaf=80, max_leaf_nodes=33, max_depth=40, max_bins=195, l2_regularization=0.03), inverse_func=Reg_for_Cla, check_inverse=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZe0b2uM66U"
      },
      "outputs": [],
      "source": [
        "### Random Search\n",
        "param_dist = {'learning_rate': (0.01,0.1,'uniform'),\n",
        "              'max_depth': (3,10),\n",
        "              'n_estimators': (100,1500),\n",
        "              'min_child_weight':(0.5,15), # the smaller, the more easy to overfit\n",
        "              'gamma': (0.001,0.5)\n",
        "              }                    \n",
        "search = BayesSearchCV(estimator=xgb_reg, param_distributions=param_dist, scoring='neg_mean_absolute_error', n_iter=50, cv=5) #Randomized / Bayes\n",
        "search.fit(x_train, y_train)\n",
        "Search_Result(search)\n",
        "#HistGB {'regressor__min_samples_leaf': 80, 'regressor__max_leaf_nodes': 33, 'regressor__max_depth': 40, 'regressor__max_bins': 195, 'regressor__l2_regularization': 0.03} 1.6626"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4-pIWBiiBEO"
      },
      "outputs": [],
      "source": [
        "### Grid Search\n",
        "param_dist = {'regressor__random_state': [1126, 6211, None],\n",
        "              'regressor__max_depth': [20, 80, 140, 200, None],                     \n",
        "              'regressor__l2_regularization': [0.01, 0.03, 0.1, 0.3, 1, 3]\n",
        "              }\n",
        "search = GridSearchCV(estimator=gbr_r, param_grid=param_dist, scoring='neg_mean_absolute_error', cv=5)\n",
        "search.fit(x_train, y_train)\n",
        "Search_Result(search)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 3:1 Validation\n",
        "x_train = x_train17\n",
        "x_train3, x_eval, y_train3, y_eval = train_test_split(x_train, y_train)\n",
        "\n",
        "xgb_model = xgb_reg.fit(x_train3, y_train3)\n",
        "y_train_pred = xgb_model.predict(x_train3)\n",
        "stump = Stump(y_train_pred, y_train3)\n",
        "\n",
        "y_pred = xgb_model.predict(x_eval)\n",
        "y_pred_int = y_pred.round()\n",
        "y_pred_stump = Stump_Apply(y_pred, stump)\n",
        "print(mean_absolute_error(y_pred, y_eval))  #1.5932098731716182\n",
        "print(mean_absolute_error(y_pred_int, y_eval))  #1.5804798509201026\n",
        "print(mean_absolute_error(y_pred_stump, y_eval))  #1.5583508036338225"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr58s6dmkB-d",
        "outputId": "adcae432-d1b6-477d-820a-b4c488d8e136"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.58528304724628\n",
            "1.5648730491497786\n",
            "1.5427440018634988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZYny5MYdRYG"
      },
      "outputs": [],
      "source": [
        "### 5-fold Cross Validation\n",
        "options = [(gbr, \"param 1\")]\n",
        "for model, msg in options:\n",
        "    cv_score = cross_val_score(model, x_train, y_train, cv=5, scoring=\"neg_mean_absolute_error\")\n",
        "    CV_Average(cv_score, msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Zs1X4TPBCT_"
      },
      "outputs": [],
      "source": [
        "### Confusion Matrix on Classifier\n",
        "x_train = x_train17\n",
        "x_train3, x_eval, y_train3, y_eval = train_test_split(x_train, y_train)\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "titles_options = [(\"Confusion matrix, without normalization\", None)]#,\n",
        "                  #(\"Normalized confusion matrix\", \"true\"),]\n",
        "for title, normalize in titles_options:\n",
        "    disp = ConfusionMatrixDisplay.from_predictions(y_eval, y_pred,\n",
        "        display_labels=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'],\n",
        "        cmap=plt.cm.Blues,\n",
        "        normalize=normalize)\n",
        "    disp.ax_.set_title(title)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5InxSfmxHWQ3"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFuNgC7wHWAu"
      },
      "outputs": [],
      "source": [
        "### load data\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/test.csv\", delimiter=\",\", header=0)\n",
        "test_df = test_df.drop(['Track','Album','Uri','Url_spotify','Url_youtube','Description','Title','Channel'], axis =1)\n",
        "n_test = test_df.shape[0]\n",
        "\n",
        "id = test_df[['id']].copy()\n",
        "id = id.to_numpy()\n",
        "submit = np.zeros((n_test, 2))\n",
        "submit[:, 0] = id[:, 0]\n",
        "\n",
        "x_test_pd17 = test_df.drop(['id','Composer','Artist'], axis =1)\n",
        "x_test_pd14 = x_test_pd17.drop(['Album_type','official_video','Licensed'], axis =1)\n",
        "\n",
        "album_map = {'album': 3, 'single': 2, 'compilation': 1}\n",
        "tf_map = {1: 1, 0: 0}\n",
        "x_test_pd17['Album_type']     = x_test_pd17['Album_type'].map(album_map, na_action='ignore')\n",
        "x_test_pd17['official_video'] = x_test_pd17['official_video'].map(tf_map, na_action='ignore')\n",
        "x_test_pd17['Licensed']       = x_test_pd17['Licensed'].map(tf_map, na_action='ignore')\n",
        "x_test_pd125 = test_df.drop(['id'], axis =1)\n",
        "x_test_pd125 = pd.get_dummies(test_df, columns=['Artist','Composer','Album_type'])    \n",
        "\n",
        "x_test17      = x_test_pd17.to_numpy()\n",
        "x_test14      = x_test_pd14.to_numpy()\n",
        "x_test125     = x_test_pd125.to_numpy()  \n",
        "#pd.set_option('display.max_columns', 500)                                      \n",
        "#print(x_test_pd17.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2FlqhzGLM8G"
      },
      "outputs": [],
      "source": [
        "### make prediction\n",
        "x_train = x_train125\n",
        "x_test  = x_test125 #choose x\n",
        "model = xgb_reg.fit(x_train, y_train) #change model name\n",
        "y_train_pred = model.predict(x_train)\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_stump = Stump_Set(y_pred, y_train_pred, y_train)\n",
        "submit[:, 1] = y_pred_stump\n",
        "#submit[:, 1] = gbr_rp.predict(x_test) #change model name again\n",
        "\n",
        "df = pd.DataFrame(submit, columns = ['id','Danceability'])\n",
        "df = df.astype({\"id\": int})\n",
        "df.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30-t3KU1nbtY"
      },
      "source": [
        "# Past Trials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjD-8BRgUHX-"
      },
      "outputs": [],
      "source": [
        "### Parameter Search\n",
        "param_dist = {'regressor__max_depth': [5, 20, 80, 140, 200, None],                     \n",
        "              'regressor__l2_regularization': [0.01, 0.03, 0.1, 0.3, 1, 3]}\n",
        "grid_search = GridSearchCV(estimator=gbr_r, param_grid=param_dist, scoring='neg_mean_absolute_error', cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "Search_Result(grid_search)\n",
        "# Result on x_train_17\n",
        "# 'l2_regularization': 3, 'max_depth': None  cv_socre: 1.6642399534071053\n",
        "# depth = 5 consistently bad\n",
        "# depth = none needs at least l2=0.03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXfKz4agngy4"
      },
      "outputs": [],
      "source": [
        "### Effect of Random State\n",
        "param_dist = {'regressor__random_state': [0, 10, 1126, 6211, 100000, None]}                     #255 (<=255)\n",
        "grid_search = GridSearchCV(estimator=gbr_r, param_grid=param_dist, scoring='neg_mean_absolute_error', cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "Search_Result(grid_search)\n",
        "# Done on x_train17\n",
        "# Result: Use None"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DqDuEVzvTDTu",
        "lqgWUGvTTOAV",
        "5InxSfmxHWQ3",
        "30-t3KU1nbtY"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
